{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What in this notebook\n",
    "\n",
    "- Running the code in `demo.ipynb` with the vulnerabilities dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.inputters.corpus import ParallelCorpus\n",
    "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
    "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
    "from onmt.utils.misc import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable logging\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "import os\n",
    "\n",
    "# Defining log path to keep track of the experiment\n",
    "LOG_PATH = 'log/log_100k_steps'\n",
    "FULL_LOG_PATH = os.getcwd() + '/' + LOG_PATH\n",
    "if os.path.exists(FULL_LOG_PATH):\n",
    "    os.remove(FULL_LOG_PATH)\n",
    "    \n",
    "init_logger(log_file=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/utils/logging.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_logger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4 # data iterater definition\n",
    "VALID_BATCH_SIZE=1 # data iterator definition\n",
    "SRC_VOCAB_SIZE=2000 # fields definition\n",
    "TGT_VOCAB_SIZE=2000 # fields definition\n",
    "SRC_SEQ_LENGTH=1000 # currently not needed as we train a dummy on preprocessed data\n",
    "TGT_SEQ_LENGTH=100 # currently not needed as we train a dummy on preprocessed data\n",
    "LEARNING_RATE=0.0005 # loss definition\n",
    "LABEL_SMOOTHING=0.1 # loss definition\n",
    "ADAM_DECAY=0.9 # loss definition\n",
    "RNN_HIDDEN=256 # model definition\n",
    "EMBEDDING=256 # model definition\n",
    "WORD_VEC=256 # model definition\n",
    "DROP_OUT=0.1 # model definition\n",
    "TRAIN_STEPS=100000\n",
    "VALID_STEPS=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the original data to bulid processing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = \"vul_data/data.vocab.src\"\n",
    "tgt_vocab_path = \"vul_data/data.vocab.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:49:30,104 INFO] Loading src vocabulary from vul_data/data.vocab.src\n",
      "[2022-06-22 02:49:30,161 INFO] Loaded src vocab has 36352 tokens.\n",
      "[2022-06-22 02:49:30,170 INFO] Loading tgt vocabulary from vul_data/data.vocab.tgt\n",
      "[2022-06-22 02:49:30,177 INFO] Loaded tgt vocab has 5924 tokens.\n"
     ]
    }
   ],
   "source": [
    "# initialize the frequency counter\n",
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 1**: Compairing the generated dictionary with the one created from the original source code of VRepair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output of VRepair log\n",
    "> [2022-05-17 20:29:10,536 INFO] Loading src vocabulary from /home/lgm/VRepair2.0/param_sweep_tgt/10_parameter_sweep/data.vocab.src \\\n",
    " [2022-05-17 20:29:10,579 INFO] Loaded src vocab has 36352 tokens. \\\n",
    "[2022-05-17 20:29:10,588 INFO] Loading tgt vocabulary from /home/lgm/VRepair2.0/param_sweep_tgt/10_parameter_sweep/data.vocab.tgt \\\n",
    "[2022-05-17 20:29:10,594 INFO] Loaded tgt vocab has 5924 tokens. \\\n",
    "[2022-05-17 20:29:10,596 INFO] Building fields with vocab in counters... \\\n",
    "[2022-05-17 20:29:10,599 INFO]  * tgt vocab size: 5004. \\\n",
    "[2022-05-17 20:29:10,631 INFO]  * src vocab size: 5002. \\\n",
    "[2022-05-17 20:29:10,632 INFO]  * src vocab size = 5002 \\\n",
    "[2022-05-17 20:29:10,632 INFO]  * tgt vocab size = 5004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS RQ1:** The vocab generated is the same with previous experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:49:31,624 INFO]  * tgt vocab size: 2004.\n",
      "[2022-06-22 02:49:31,644 INFO]  * src vocab size: 2002.\n"
     ]
    }
   ],
   "source": [
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = SRC_VOCAB_SIZE\n",
    "tgt_vocab_size = TGT_VOCAB_SIZE\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0m_build_fields_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcounters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshare_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocab_size_multiple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_words_min_frequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_words_min_frequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_specials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_specials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/inputters/inputter.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_build_fields_vocab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and optimizer creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the field vocab is used instead of the origial vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab \n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMTLossCompute(\n",
       "  (criterion): LabelSmoothingLoss()\n",
       "  (generator): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2004, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = EMBEDDING\n",
    "rnn_size = RNN_HIDDEN\n",
    "# Specify the core model.\n",
    "\n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
    "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
    "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=onmt.utils.loss.LabelSmoothingLoss(ignore_index=tgt_padding, label_smoothing=LABEL_SMOOTHING, tgt_vocab_size=len(tgt_vocab)),\n",
    "    generator=model.generator)\n",
    "loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from onmt.opts import dynamic_prepare_opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from onmt.constants import ModelTask\n",
    "parser = ArgumentParser(description='build_loss_compute')\n",
    "\n",
    "base_args = ([\"-copy_attn\", \"True\" , \"-label_smoothing\", str(LABEL_SMOOTHING), \"-model_task\", ModelTask.SEQ2SEQ])\n",
    "opts, unknown = parser.parse_known_args(base_args)\n",
    "loss = onmt.utils.loss.build_loss_compute(model, tgt_field=tgt_text_field, opt=opts, train=True)\n",
    "valid_loss = onmt.utils.loss.build_loss_compute(model, tgt_field=tgt_text_field, opt=opts, train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LEARNING_RATE\n",
    "torch_optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=ADAM_DECAY)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(2002, 256, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(256, 128, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(2004, 256, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 256)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (linear_out): Linear(in_features=512, out_features=256, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=2004, bias=True)\n",
      "    (1): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"vul_data/random_fine_tune_train.src.txt\"\n",
    "tgt_train = \"vul_data/random_fine_tune_train.tgt.txt\"\n",
    "src_val = \"vul_data/random_fine_tune_valid.src.txt\"\n",
    "tgt_val = \"vul_data/random_fine_tune_valid.tgt.txt\"\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(\"corpus\", src_train, tgt_train)\n",
    "valid = ParallelCorpus(\"valid\", src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the training iterator\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora={\"corpus\": corpus},\n",
    "    corpora_info={\"corpus\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=True,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDynamicDatasetIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpora_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size_multiple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbucket_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpool_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_empty_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Yield batch from (multiple) plain text corpus.\n",
       "\n",
       "Args:\n",
       "    corpora (dict[str, ParallelCorpus]): collections of corpora to iterate;\n",
       "    corpora_info (dict[str, dict]): corpora infos correspond to corpora;\n",
       "    transforms (dict[str, Transform]): transforms may be used by corpora;\n",
       "    fields (dict[str, Field]): fields dict for convert corpora into Tensor;\n",
       "    is_train (bool): True when generate data for training;\n",
       "    batch_type (str): batching type to count on, choices=[tokens, sents];\n",
       "    batch_size (int): numbers of examples in a batch;\n",
       "    batch_size_multiple (int): make batch size multiply of this;\n",
       "    data_type (str): input data type, currently only text;\n",
       "    bucket_size (int): accum this number of examples in a dynamic dataset;\n",
       "    pool_factor (int): accum this number of batch before sorting;\n",
       "    skip_empty_level (str): security level when encouter empty line;\n",
       "    stride (int): iterate data files with this stride;\n",
       "    offset (int): iterate data files with this offset.\n",
       "\n",
       "Attributes:\n",
       "    batch_size_fn (function): functions to calculate batch_size;\n",
       "    sort_key (function): functions define how to sort examples;\n",
       "    dataset_adapter (DatasetAdapter): organize raw corpus to tensor adapt;\n",
       "    mixer (MixingStrategy): the strategy to iterate corpora.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/inputters/dynamic_iterator.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DynamicDatasetIter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
    "train_iter = iter(IterOnDevice(train_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation iterator\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora={\"valid\": valid},\n",
    "    corpora_info={\"valid\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=False,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = IterOnDevice(valid_iter, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:53:18,910 INFO] Start training loop and validate every 20000 steps...\n",
      "[2022-06-22 02:53:18,911 INFO] corpus's transforms: TransformPipe()\n",
      "[2022-06-22 02:53:18,911 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 1\n",
      "[2022-06-22 02:53:18,989 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:19,829 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:19,966 INFO] Step 1100/100000; acc:  12.49; ppl: 37.76; xent: 3.63; lr: 0.00050; 9715/956 tok/s;      1 sec\n",
      "[2022-06-22 02:53:20,873 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:21,392 INFO] Step 1150/100000; acc:   9.81; ppl: 51.54; xent: 3.94; lr: 0.00050; 9709/1117 tok/s;      2 sec\n",
      "[2022-06-22 02:53:21,765 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:22,691 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:22,755 INFO] Step 1200/100000; acc:   9.32; ppl: 52.18; xent: 3.95; lr: 0.00050; 12722/984 tok/s;      4 sec\n",
      "[2022-06-22 02:53:23,714 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:24,216 INFO] Step 1250/100000; acc:  11.85; ppl: 48.78; xent: 3.89; lr: 0.00050; 11578/1046 tok/s;      5 sec\n",
      "[2022-06-22 02:53:24,718 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:25,486 INFO] Step 1300/100000; acc:  12.23; ppl: 48.89; xent: 3.89; lr: 0.00050; 10277/1076 tok/s;      7 sec\n",
      "[2022-06-22 02:53:25,682 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:26,451 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:26,780 INFO] Step 1350/100000; acc:   9.45; ppl: 49.63; xent: 3.90; lr: 0.00050; 11249/1039 tok/s;      8 sec\n",
      "[2022-06-22 02:53:27,484 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:28,175 INFO] Step 1400/100000; acc:  10.39; ppl: 54.93; xent: 4.01; lr: 0.00050; 12146/1015 tok/s;      9 sec\n",
      "[2022-06-22 02:53:28,472 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:29,435 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:29,473 INFO] Step 1450/100000; acc:  11.62; ppl: 44.62; xent: 3.80; lr: 0.00050; 10613/1062 tok/s;     11 sec\n",
      "[2022-06-22 02:53:30,559 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:30,736 INFO] Step 1500/100000; acc:  10.14; ppl: 48.49; xent: 3.88; lr: 0.00050; 9199/1109 tok/s;     12 sec\n",
      "[2022-06-22 02:53:31,653 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:32,091 INFO] Step 1550/100000; acc:   9.37; ppl: 48.14; xent: 3.87; lr: 0.00050; 10771/1055 tok/s;     13 sec\n",
      "[2022-06-22 02:53:32,714 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:33,343 INFO] Step 1600/100000; acc:  11.01; ppl: 48.00; xent: 3.87; lr: 0.00050; 9945/1074 tok/s;     14 sec\n",
      "[2022-06-22 02:53:33,589 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:34,506 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:34,632 INFO] Step 1650/100000; acc:  11.54; ppl: 43.47; xent: 3.77; lr: 0.00050; 11767/1009 tok/s;     16 sec\n",
      "[2022-06-22 02:53:35,528 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:35,999 INFO] Step 1700/100000; acc:  10.75; ppl: 47.64; xent: 3.86; lr: 0.00050; 12727/980 tok/s;     17 sec\n",
      "[2022-06-22 02:53:36,486 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:37,239 INFO] Step 1750/100000; acc:  11.36; ppl: 41.10; xent: 3.72; lr: 0.00050; 10614/1044 tok/s;     18 sec\n",
      "[2022-06-22 02:53:37,349 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:38,661 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:38,781 INFO] Step 1800/100000; acc:  10.24; ppl: 53.67; xent: 3.98; lr: 0.00050; 9204/1140 tok/s;     20 sec\n",
      "[2022-06-22 02:53:39,423 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:40,125 INFO] Step 1850/100000; acc:  11.25; ppl: 55.17; xent: 4.01; lr: 0.00050; 11783/1012 tok/s;     21 sec\n",
      "[2022-06-22 02:53:40,598 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:41,403 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:41,417 INFO] Step 1900/100000; acc:  11.56; ppl: 51.28; xent: 3.94; lr: 0.00050; 11469/1019 tok/s;     23 sec\n",
      "[2022-06-22 02:53:42,249 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:42,642 INFO] Step 1950/100000; acc:  11.04; ppl: 48.11; xent: 3.87; lr: 0.00050; 9824/1080 tok/s;     24 sec\n",
      "[2022-06-22 02:53:43,382 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:44,087 INFO] Step 2000/100000; acc:  12.48; ppl: 48.86; xent: 3.89; lr: 0.00050; 10931/1059 tok/s;     25 sec\n",
      "[2022-06-22 02:53:44,613 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:45,501 INFO] Step 2050/100000; acc:  11.45; ppl: 54.21; xent: 3.99; lr: 0.00050; 9807/1106 tok/s;     27 sec\n",
      "[2022-06-22 02:53:45,502 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:46,411 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:46,838 INFO] Step 2100/100000; acc:  11.67; ppl: 50.91; xent: 3.93; lr: 0.00050; 10056/1084 tok/s;     28 sec\n",
      "[2022-06-22 02:53:47,612 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:48,405 INFO] Step 2150/100000; acc:  10.78; ppl: 45.97; xent: 3.83; lr: 0.00050; 10053/1113 tok/s;     29 sec\n",
      "[2022-06-22 02:53:48,444 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:49,323 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:49,717 INFO] Step 2200/100000; acc:  11.09; ppl: 46.14; xent: 3.83; lr: 0.00050; 11028/1039 tok/s;     31 sec\n",
      "[2022-06-22 02:53:50,485 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:51,027 INFO] Step 2250/100000; acc:  11.36; ppl: 47.14; xent: 3.85; lr: 0.00050; 11262/1029 tok/s;     32 sec\n",
      "[2022-06-22 02:53:51,486 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:52,307 INFO] Step 2300/100000; acc:  11.66; ppl: 47.35; xent: 3.86; lr: 0.00050; 10183/1072 tok/s;     33 sec\n",
      "[2022-06-22 02:53:52,331 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:53,359 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:53,640 INFO] Step 2350/100000; acc:  10.93; ppl: 54.94; xent: 4.01; lr: 0.00050; 10788/1051 tok/s;     35 sec\n",
      "[2022-06-22 02:53:54,603 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:54,935 INFO] Step 2400/100000; acc:  11.00; ppl: 46.74; xent: 3.84; lr: 0.00050; 9552/1095 tok/s;     36 sec\n",
      "[2022-06-22 02:53:55,621 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:56,551 INFO] Step 2450/100000; acc:  10.15; ppl: 46.18; xent: 3.83; lr: 0.00050; 10505/1097 tok/s;     38 sec\n",
      "[2022-06-22 02:53:56,736 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:57,582 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:57,895 INFO] Step 2500/100000; acc:  10.27; ppl: 51.52; xent: 3.94; lr: 0.00050; 8181/1160 tok/s;     39 sec\n",
      "[2022-06-22 02:53:58,432 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:53:59,335 INFO] Step 2550/100000; acc:  10.91; ppl: 48.91; xent: 3.89; lr: 0.00050; 11537/1038 tok/s;     40 sec\n",
      "[2022-06-22 02:53:59,387 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:00,263 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:00,525 INFO] Step 2600/100000; acc:  12.55; ppl: 44.46; xent: 3.79; lr: 0.00050; 11066/1018 tok/s;     42 sec\n",
      "[2022-06-22 02:54:01,178 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:01,953 INFO] Step 2650/100000; acc:  12.39; ppl: 45.61; xent: 3.82; lr: 0.00050; 11302/1046 tok/s;     43 sec\n",
      "[2022-06-22 02:54:02,115 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:03,328 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:03,372 INFO] Step 2700/100000; acc:  10.67; ppl: 52.17; xent: 3.95; lr: 0.00050; 9331/1117 tok/s;     44 sec\n",
      "[2022-06-22 02:54:04,265 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:04,598 INFO] Step 2750/100000; acc:  10.34; ppl: 51.76; xent: 3.95; lr: 0.00050; 10426/1049 tok/s;     46 sec\n",
      "[2022-06-22 02:54:05,187 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:05,781 INFO] Step 2800/100000; acc:  10.92; ppl: 49.04; xent: 3.89; lr: 0.00050; 10912/1023 tok/s;     47 sec\n",
      "[2022-06-22 02:54:06,102 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:07,228 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:07,242 INFO] Step 2850/100000; acc:   9.66; ppl: 50.85; xent: 3.93; lr: 0.00050; 10154/1091 tok/s;     48 sec\n",
      "[2022-06-22 02:54:08,134 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:08,662 INFO] Step 2900/100000; acc:  10.59; ppl: 48.82; xent: 3.89; lr: 0.00050; 12368/998 tok/s;     50 sec\n",
      "[2022-06-22 02:54:09,282 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:10,014 INFO] Step 2950/100000; acc:  11.05; ppl: 51.69; xent: 3.95; lr: 0.00050; 10096/1085 tok/s;     51 sec\n",
      "[2022-06-22 02:54:10,362 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:11,220 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:11,331 INFO] Step 3000/100000; acc:  10.46; ppl: 46.37; xent: 3.84; lr: 0.00050; 11839/1002 tok/s;     52 sec\n",
      "[2022-06-22 02:54:12,091 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:12,658 INFO] Step 3050/100000; acc:  11.71; ppl: 51.49; xent: 3.94; lr: 0.00050; 12121/998 tok/s;     54 sec\n",
      "[2022-06-22 02:54:13,329 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:13,964 INFO] Step 3100/100000; acc:  12.61; ppl: 47.11; xent: 3.85; lr: 0.00050; 10827/1058 tok/s;     55 sec\n",
      "[2022-06-22 02:54:14,269 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 2\n",
      "[2022-06-22 02:54:14,475 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:15,454 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:15,530 INFO] Step 3150/100000; acc:  11.47; ppl: 48.20; xent: 3.88; lr: 0.00050; 9734/897 tok/s;     57 sec\n",
      "[2022-06-22 02:54:16,269 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:16,932 INFO] Step 3200/100000; acc:  11.22; ppl: 51.52; xent: 3.94; lr: 0.00050; 11671/1031 tok/s;     58 sec\n",
      "[2022-06-22 02:54:17,647 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:18,278 INFO] Step 3250/100000; acc:  10.40; ppl: 51.09; xent: 3.93; lr: 0.00050; 9018/1136 tok/s;     59 sec\n",
      "[2022-06-22 02:54:18,604 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:19,511 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:19,623 INFO] Step 3300/100000; acc:  11.20; ppl: 51.74; xent: 3.95; lr: 0.00050; 10930/1056 tok/s;     61 sec\n",
      "[2022-06-22 02:54:20,483 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:20,902 INFO] Step 3350/100000; acc:  12.26; ppl: 44.94; xent: 3.81; lr: 0.00050; 11445/1027 tok/s;     62 sec\n",
      "[2022-06-22 02:54:21,682 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:22,341 INFO] Step 3400/100000; acc:   8.83; ppl: 56.72; xent: 4.04; lr: 0.00050; 8358/1173 tok/s;     63 sec\n",
      "[2022-06-22 02:54:22,661 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:23,474 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:23,706 INFO] Step 3450/100000; acc:  11.82; ppl: 44.74; xent: 3.80; lr: 0.00050; 11281/1042 tok/s;     65 sec\n",
      "[2022-06-22 02:54:24,414 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:25,058 INFO] Step 3500/100000; acc:  10.83; ppl: 44.71; xent: 3.80; lr: 0.00050; 10327/1079 tok/s;     66 sec\n",
      "[2022-06-22 02:54:25,249 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:26,256 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:26,365 INFO] Step 3550/100000; acc:  11.27; ppl: 47.46; xent: 3.86; lr: 0.00050; 11599/1026 tok/s;     67 sec\n",
      "[2022-06-22 02:54:27,274 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:27,641 INFO] Step 3600/100000; acc:  10.71; ppl: 48.53; xent: 3.88; lr: 0.00050; 11693/1018 tok/s;     69 sec\n",
      "[2022-06-22 02:54:28,359 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:28,967 INFO] Step 3650/100000; acc:  10.41; ppl: 48.25; xent: 3.88; lr: 0.00050; 10333/1080 tok/s;     70 sec\n",
      "[2022-06-22 02:54:29,237 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:30,193 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:30,282 INFO] Step 3700/100000; acc:  11.10; ppl: 55.25; xent: 4.01; lr: 0.00050; 12329/993 tok/s;     71 sec\n",
      "[2022-06-22 02:54:31,182 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:31,620 INFO] Step 3750/100000; acc:  11.17; ppl: 48.20; xent: 3.88; lr: 0.00050; 10978/1058 tok/s;     73 sec\n",
      "[2022-06-22 02:54:32,011 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:32,928 INFO] Step 3800/100000; acc:  10.91; ppl: 43.91; xent: 3.78; lr: 0.00050; 11276/1037 tok/s;     74 sec\n",
      "[2022-06-22 02:54:33,197 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:34,297 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:34,450 INFO] Step 3850/100000; acc:  10.03; ppl: 57.58; xent: 4.05; lr: 0.00050; 8314/1179 tok/s;     76 sec\n",
      "[2022-06-22 02:54:35,286 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:35,729 INFO] Step 3900/100000; acc:  12.34; ppl: 44.56; xent: 3.80; lr: 0.00050; 10311/1072 tok/s;     77 sec\n",
      "[2022-06-22 02:54:36,169 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:37,147 INFO] Step 3950/100000; acc:  10.82; ppl: 51.40; xent: 3.94; lr: 0.00050; 11440/1044 tok/s;     78 sec\n",
      "[2022-06-22 02:54:37,206 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:38,186 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:38,421 INFO] Step 4000/100000; acc:   8.84; ppl: 55.28; xent: 4.01; lr: 0.00050; 9042/1120 tok/s;     80 sec\n",
      "[2022-06-22 02:54:39,008 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:39,602 INFO] Step 4050/100000; acc:  11.13; ppl: 50.85; xent: 3.93; lr: 0.00050; 11878/990 tok/s;     81 sec\n",
      "[2022-06-22 02:54:39,904 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:40,848 INFO] Step 4100/100000; acc:  10.83; ppl: 50.55; xent: 3.92; lr: 0.00050; 10044/1076 tok/s;     82 sec\n",
      "[2022-06-22 02:54:40,849 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:41,889 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:42,256 INFO] Step 4150/100000; acc:  10.17; ppl: 47.38; xent: 3.86; lr: 0.00050; 9855/1097 tok/s;     83 sec\n",
      "[2022-06-22 02:54:42,942 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:43,621 INFO] Step 4200/100000; acc:  12.96; ppl: 45.80; xent: 3.82; lr: 0.00050; 11888/1018 tok/s;     85 sec\n",
      "[2022-06-22 02:54:43,879 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:44,694 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:44,774 INFO] Step 4250/100000; acc:  11.70; ppl: 54.16; xent: 3.99; lr: 0.00050; 12015/979 tok/s;     86 sec\n",
      "[2022-06-22 02:54:45,676 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:46,131 INFO] Step 4300/100000; acc:   9.80; ppl: 48.90; xent: 3.89; lr: 0.00050; 11654/1024 tok/s;     87 sec\n",
      "[2022-06-22 02:54:46,570 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:47,562 INFO] Step 4350/100000; acc:  11.81; ppl: 49.66; xent: 3.91; lr: 0.00050; 12354/1005 tok/s;     89 sec\n",
      "[2022-06-22 02:54:47,797 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:48,635 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:48,832 INFO] Step 4400/100000; acc:  11.56; ppl: 45.34; xent: 3.81; lr: 0.00050; 11281/1029 tok/s;     90 sec\n",
      "[2022-06-22 02:54:49,432 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:50,143 INFO] Step 4450/100000; acc:  11.40; ppl: 53.66; xent: 3.98; lr: 0.00050; 10268/1078 tok/s;     91 sec\n",
      "[2022-06-22 02:54:50,474 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:51,381 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:51,422 INFO] Step 4500/100000; acc:  11.05; ppl: 48.43; xent: 3.88; lr: 0.00050; 9451/1105 tok/s;     93 sec\n",
      "[2022-06-22 02:54:52,308 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:52,929 INFO] Step 4550/100000; acc:  10.26; ppl: 53.16; xent: 3.97; lr: 0.00050; 10813/1080 tok/s;     94 sec\n",
      "[2022-06-22 02:54:53,317 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:54,267 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:54,373 INFO] Step 4600/100000; acc:  10.23; ppl: 45.95; xent: 3.83; lr: 0.00050; 11889/1023 tok/s;     95 sec\n",
      "[2022-06-22 02:54:55,179 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:55,901 INFO] Step 4650/100000; acc:   9.26; ppl: 45.06; xent: 3.81; lr: 0.00050; 10919/1075 tok/s;     97 sec\n",
      "[2022-06-22 02:54:56,208 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:57,306 INFO] Step 4700/100000; acc:  11.94; ppl: 49.93; xent: 3.91; lr: 0.00050; 9512/1121 tok/s;     98 sec\n",
      "[2022-06-22 02:54:57,307 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:58,273 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:54:58,707 INFO] Step 4750/100000; acc:  11.06; ppl: 48.02; xent: 3.87; lr: 0.00050; 11244/1040 tok/s;    100 sec\n",
      "[2022-06-22 02:54:59,251 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:00,099 INFO] Step 4800/100000; acc:  11.19; ppl: 50.52; xent: 3.92; lr: 0.00050; 10601/1072 tok/s;    101 sec\n",
      "[2022-06-22 02:55:00,254 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:01,109 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:01,360 INFO] Step 4850/100000; acc:  11.05; ppl: 50.68; xent: 3.93; lr: 0.00050; 11172/1005 tok/s;    102 sec\n",
      "[2022-06-22 02:55:02,109 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:02,795 INFO] Step 4900/100000; acc:  11.75; ppl: 48.22; xent: 3.88; lr: 0.00050; 9001/1097 tok/s;    104 sec\n",
      "[2022-06-22 02:55:02,998 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:04,058 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:04,115 INFO] Step 4950/100000; acc:  12.02; ppl: 50.63; xent: 3.92; lr: 0.00050; 11143/1034 tok/s;    105 sec\n",
      "[2022-06-22 02:55:04,992 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:05,454 INFO] Step 5000/100000; acc:  10.52; ppl: 54.81; xent: 4.00; lr: 0.00050; 9666/1101 tok/s;    107 sec\n",
      "[2022-06-22 02:55:06,133 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:06,779 INFO] Step 5050/100000; acc:  11.41; ppl: 51.86; xent: 3.95; lr: 0.00050; 10198/1078 tok/s;    108 sec\n",
      "[2022-06-22 02:55:07,215 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:08,253 INFO] Step 5100/100000; acc:  10.42; ppl: 49.48; xent: 3.90; lr: 0.00050; 9545/1121 tok/s;    109 sec\n",
      "[2022-06-22 02:55:08,444 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:09,313 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:09,601 INFO] Step 5150/100000; acc:  11.52; ppl: 46.23; xent: 3.83; lr: 0.00050; 9977/1088 tok/s;    111 sec\n",
      "[2022-06-22 02:55:09,876 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 3\n",
      "[2022-06-22 02:55:09,950 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:11,093 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:11,137 INFO] Step 5200/100000; acc:   9.78; ppl: 50.64; xent: 3.92; lr: 0.00050; 9502/1025 tok/s;    112 sec\n",
      "[2022-06-22 02:55:11,983 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:12,529 INFO] Step 5250/100000; acc:  10.22; ppl: 49.74; xent: 3.91; lr: 0.00050; 10977/1048 tok/s;    114 sec\n",
      "[2022-06-22 02:55:12,996 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:13,872 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:14,002 INFO] Step 5300/100000; acc:  11.83; ppl: 50.58; xent: 3.92; lr: 0.00050; 11581/1033 tok/s;    115 sec\n",
      "[2022-06-22 02:55:14,982 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:15,367 INFO] Step 5350/100000; acc:  11.20; ppl: 48.84; xent: 3.89; lr: 0.00050; 9805/1093 tok/s;    116 sec\n",
      "[2022-06-22 02:55:16,107 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:16,774 INFO] Step 5400/100000; acc:  10.28; ppl: 48.65; xent: 3.88; lr: 0.00050; 9622/1107 tok/s;    118 sec\n",
      "[2022-06-22 02:55:17,184 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:18,158 INFO] Step 5450/100000; acc:  11.18; ppl: 53.39; xent: 3.98; lr: 0.00050; 10176/1080 tok/s;    119 sec\n",
      "[2022-06-22 02:55:18,310 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:19,161 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:19,324 INFO] Step 5500/100000; acc:  11.82; ppl: 51.65; xent: 3.94; lr: 0.00050; 12572/950 tok/s;    120 sec\n",
      "[2022-06-22 02:55:20,123 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:20,610 INFO] Step 5550/100000; acc:  11.14; ppl: 44.17; xent: 3.79; lr: 0.00050; 10589/1055 tok/s;    122 sec\n",
      "[2022-06-22 02:55:20,984 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:21,796 INFO] Step 5600/100000; acc:  10.94; ppl: 43.00; xent: 3.76; lr: 0.00050; 10916/1026 tok/s;    123 sec\n",
      "[2022-06-22 02:55:21,797 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:22,952 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:23,038 INFO] Step 5650/100000; acc:  10.83; ppl: 54.74; xent: 4.00; lr: 0.00050; 9204/1101 tok/s;    124 sec\n",
      "[2022-06-22 02:55:23,980 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:24,300 INFO] Step 5700/100000; acc:  10.81; ppl: 44.30; xent: 3.79; lr: 0.00050; 9229/1099 tok/s;    125 sec\n",
      "[2022-06-22 02:55:24,954 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:25,703 INFO] Step 5750/100000; acc:  10.21; ppl: 49.84; xent: 3.91; lr: 0.00050; 9550/1104 tok/s;    127 sec\n",
      "[2022-06-22 02:55:25,970 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:26,992 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:27,051 INFO] Step 5800/100000; acc:  10.50; ppl: 45.94; xent: 3.83; lr: 0.00050; 11819/1004 tok/s;    128 sec\n",
      "[2022-06-22 02:55:27,842 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:28,465 INFO] Step 5850/100000; acc:  10.59; ppl: 53.75; xent: 3.98; lr: 0.00050; 10190/1082 tok/s;    130 sec\n",
      "[2022-06-22 02:55:28,868 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:29,986 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:30,145 INFO] Step 5900/100000; acc:  11.20; ppl: 50.73; xent: 3.93; lr: 0.00050; 10834/973 tok/s;    131 sec\n",
      "[2022-06-22 02:55:31,117 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:31,564 INFO] Step 5950/100000; acc:   9.60; ppl: 52.22; xent: 3.96; lr: 0.00050; 11427/1035 tok/s;    133 sec\n",
      "[2022-06-22 02:55:32,152 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:33,075 INFO] Step 6000/100000; acc:  11.69; ppl: 49.36; xent: 3.90; lr: 0.00050; 9846/1111 tok/s;    134 sec\n",
      "[2022-06-22 02:55:33,265 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:34,258 INFO] Step 6050/100000; acc:  10.86; ppl: 49.59; xent: 3.90; lr: 0.00050; 10996/1020 tok/s;    135 sec\n",
      "[2022-06-22 02:55:34,294 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:35,355 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:35,685 INFO] Step 6100/100000; acc:  10.54; ppl: 50.04; xent: 3.91; lr: 0.00050; 10597/1064 tok/s;    137 sec\n",
      "[2022-06-22 02:55:36,363 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:37,097 INFO] Step 6150/100000; acc:   9.26; ppl: 61.76; xent: 4.12; lr: 0.00050; 9049/1125 tok/s;    138 sec\n",
      "[2022-06-22 02:55:37,430 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:38,287 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:38,457 INFO] Step 6200/100000; acc:  10.72; ppl: 50.63; xent: 3.92; lr: 0.00050; 13035/954 tok/s;    140 sec\n",
      "[2022-06-22 02:55:39,162 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:39,787 INFO] Step 6250/100000; acc:  11.05; ppl: 49.09; xent: 3.89; lr: 0.00050; 12553/973 tok/s;    141 sec\n",
      "[2022-06-22 02:55:40,185 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:41,218 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:41,317 INFO] Step 6300/100000; acc:  10.46; ppl: 52.58; xent: 3.96; lr: 0.00050; 9838/1107 tok/s;    142 sec\n",
      "[2022-06-22 02:55:42,207 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:42,704 INFO] Step 6350/100000; acc:   9.93; ppl: 51.30; xent: 3.94; lr: 0.00050; 9580/1104 tok/s;    144 sec\n",
      "[2022-06-22 02:55:43,204 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:43,987 INFO] Step 6400/100000; acc:  12.21; ppl: 48.03; xent: 3.87; lr: 0.00050; 11330/1015 tok/s;    145 sec\n",
      "[2022-06-22 02:55:44,312 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:45,187 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:45,365 INFO] Step 6450/100000; acc:  11.19; ppl: 52.64; xent: 3.96; lr: 0.00050; 9391/1116 tok/s;    146 sec\n",
      "[2022-06-22 02:55:46,183 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:46,657 INFO] Step 6500/100000; acc:  12.65; ppl: 51.78; xent: 3.95; lr: 0.00050; 10538/1059 tok/s;    148 sec\n",
      "[2022-06-22 02:55:47,136 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:48,045 INFO] Step 6550/100000; acc:  11.89; ppl: 46.97; xent: 3.85; lr: 0.00050; 10845/1056 tok/s;    149 sec\n",
      "[2022-06-22 02:55:48,067 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:48,977 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:49,401 INFO] Step 6600/100000; acc:  11.60; ppl: 47.35; xent: 3.86; lr: 0.00050; 10764/1056 tok/s;    150 sec\n",
      "[2022-06-22 02:55:49,798 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:50,847 INFO] Step 6650/100000; acc:  11.41; ppl: 43.78; xent: 3.78; lr: 0.00050; 9061/1134 tok/s;    152 sec\n",
      "[2022-06-22 02:55:51,158 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:51,962 INFO] Step 6700/100000; acc:  11.83; ppl: 45.66; xent: 3.82; lr: 0.00050; 12097/949 tok/s;    153 sec\n",
      "[2022-06-22 02:55:52,043 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:52,926 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:53,314 INFO] Step 6750/100000; acc:   9.17; ppl: 48.25; xent: 3.88; lr: 0.00050; 10737/1056 tok/s;    154 sec\n",
      "[2022-06-22 02:55:53,860 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:54,532 INFO] Step 6800/100000; acc:  11.71; ppl: 46.80; xent: 3.85; lr: 0.00050; 11867/989 tok/s;    156 sec\n",
      "[2022-06-22 02:55:54,707 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:55,687 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:55,829 INFO] Step 6850/100000; acc:  11.30; ppl: 48.78; xent: 3.89; lr: 0.00050; 11579/1018 tok/s;    157 sec\n",
      "[2022-06-22 02:55:56,748 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:57,327 INFO] Step 6900/100000; acc:  11.59; ppl: 46.52; xent: 3.84; lr: 0.00050; 12489/1008 tok/s;    158 sec\n",
      "[2022-06-22 02:55:57,755 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:58,822 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:55:58,925 INFO] Step 6950/100000; acc:   8.93; ppl: 50.03; xent: 3.91; lr: 0.00050; 9306/1142 tok/s;    160 sec\n",
      "[2022-06-22 02:55:59,627 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:00,067 INFO] Step 7000/100000; acc:  12.43; ppl: 44.32; xent: 3.79; lr: 0.00050; 12228/965 tok/s;    161 sec\n",
      "[2022-06-22 02:56:00,620 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:01,392 INFO] Step 7050/100000; acc:  10.24; ppl: 51.99; xent: 3.95; lr: 0.00050; 9335/1113 tok/s;    162 sec\n",
      "[2022-06-22 02:56:01,554 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:02,586 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:02,801 INFO] Step 7100/100000; acc:  10.87; ppl: 45.46; xent: 3.82; lr: 0.00050; 10529/1072 tok/s;    164 sec\n",
      "[2022-06-22 02:56:03,555 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:04,224 INFO] Step 7150/100000; acc:   8.75; ppl: 53.72; xent: 3.98; lr: 0.00050; 9871/1101 tok/s;    165 sec\n",
      "[2022-06-22 02:56:04,520 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:05,542 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:05,707 INFO] Step 7200/100000; acc:  10.41; ppl: 49.78; xent: 3.91; lr: 0.00050; 11805/1030 tok/s;    167 sec\n",
      "[2022-06-22 02:56:05,874 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 4\n",
      "[2022-06-22 02:56:05,943 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:07,078 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:07,175 INFO] Step 7250/100000; acc:  10.96; ppl: 44.17; xent: 3.79; lr: 0.00050; 8082/1082 tok/s;    168 sec\n",
      "[2022-06-22 02:56:08,022 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:08,659 INFO] Step 7300/100000; acc:  11.87; ppl: 49.31; xent: 3.90; lr: 0.00050; 12029/1022 tok/s;    170 sec\n",
      "[2022-06-22 02:56:09,183 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:09,945 INFO] Step 7350/100000; acc:  11.36; ppl: 48.08; xent: 3.87; lr: 0.00050; 12148/993 tok/s;    171 sec\n",
      "[2022-06-22 02:56:10,064 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:11,329 INFO] Step 7400/100000; acc:  11.97; ppl: 51.16; xent: 3.93; lr: 0.00050; 9873/1100 tok/s;    172 sec\n",
      "[2022-06-22 02:56:11,330 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:12,172 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:12,615 INFO] Step 7450/100000; acc:  12.72; ppl: 42.44; xent: 3.75; lr: 0.00050; 11405/1022 tok/s;    174 sec\n",
      "[2022-06-22 02:56:13,159 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:13,815 INFO] Step 7500/100000; acc:   9.95; ppl: 52.29; xent: 3.96; lr: 0.00050; 11136/1022 tok/s;    175 sec\n",
      "[2022-06-22 02:56:13,990 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:15,100 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:15,311 INFO] Step 7550/100000; acc:  11.66; ppl: 44.12; xent: 3.79; lr: 0.00050; 8943/1147 tok/s;    176 sec\n",
      "[2022-06-22 02:56:15,970 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:16,661 INFO] Step 7600/100000; acc:  12.23; ppl: 50.82; xent: 3.93; lr: 0.00050; 10641/1061 tok/s;    178 sec\n",
      "[2022-06-22 02:56:17,004 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:17,951 INFO] Step 7650/100000; acc:  11.37; ppl: 48.60; xent: 3.88; lr: 0.00050; 10565/1057 tok/s;    179 sec\n",
      "[2022-06-22 02:56:18,131 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:19,288 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:19,454 INFO] Step 7700/100000; acc:   9.00; ppl: 57.53; xent: 4.05; lr: 0.00050; 10300/1088 tok/s;    181 sec\n",
      "[2022-06-22 02:56:20,099 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:20,644 INFO] Step 7750/100000; acc:  11.58; ppl: 49.68; xent: 3.91; lr: 0.00050; 10501/1046 tok/s;    182 sec\n",
      "[2022-06-22 02:56:21,168 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:21,967 INFO] Step 7800/100000; acc:  10.75; ppl: 50.14; xent: 3.91; lr: 0.00050; 11088/1041 tok/s;    183 sec\n",
      "[2022-06-22 02:56:22,308 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:23,302 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:23,487 INFO] Step 7850/100000; acc:  11.58; ppl: 48.39; xent: 3.88; lr: 0.00050; 9930/1109 tok/s;    185 sec\n",
      "[2022-06-22 02:56:24,159 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:24,796 INFO] Step 7900/100000; acc:  12.35; ppl: 53.04; xent: 3.97; lr: 0.00050; 10459/1064 tok/s;    186 sec\n",
      "[2022-06-22 02:56:25,028 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:26,199 INFO] Step 7950/100000; acc:   9.91; ppl: 51.34; xent: 3.94; lr: 0.00050; 10561/1065 tok/s;    187 sec\n",
      "[2022-06-22 02:56:26,200 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:27,290 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:27,733 INFO] Step 8000/100000; acc:  10.46; ppl: 45.26; xent: 3.81; lr: 0.00050; 11586/1042 tok/s;    189 sec\n",
      "[2022-06-22 02:56:28,192 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:28,961 INFO] Step 8050/100000; acc:  11.63; ppl: 43.66; xent: 3.78; lr: 0.00050; 11578/1008 tok/s;    190 sec\n",
      "[2022-06-22 02:56:29,125 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:30,265 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:30,516 INFO] Step 8100/100000; acc:  10.52; ppl: 54.99; xent: 4.01; lr: 0.00050; 8883/1156 tok/s;    192 sec\n",
      "[2022-06-22 02:56:31,221 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:31,883 INFO] Step 8150/100000; acc:   9.99; ppl: 51.07; xent: 3.93; lr: 0.00050; 10173/1084 tok/s;    193 sec\n",
      "[2022-06-22 02:56:32,315 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:33,215 INFO] Step 8200/100000; acc:   9.95; ppl: 50.19; xent: 3.92; lr: 0.00050; 9104/1125 tok/s;    194 sec\n",
      "[2022-06-22 02:56:33,546 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:34,477 INFO] Step 8250/100000; acc:   9.57; ppl: 55.16; xent: 4.01; lr: 0.00050; 9022/1119 tok/s;    196 sec\n",
      "[2022-06-22 02:56:34,561 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:35,430 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:35,834 INFO] Step 8300/100000; acc:  10.95; ppl: 50.82; xent: 3.93; lr: 0.00050; 12261/996 tok/s;    197 sec\n",
      "[2022-06-22 02:56:36,415 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:37,184 INFO] Step 8350/100000; acc:  10.86; ppl: 54.42; xent: 4.00; lr: 0.00050; 11748/1017 tok/s;    198 sec\n",
      "[2022-06-22 02:56:37,262 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:38,322 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:38,641 INFO] Step 8400/100000; acc:  10.31; ppl: 51.50; xent: 3.94; lr: 0.00050; 11211/1052 tok/s;    200 sec\n",
      "[2022-06-22 02:56:39,232 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:39,858 INFO] Step 8450/100000; acc:  10.28; ppl: 54.40; xent: 4.00; lr: 0.00050; 10764/1039 tok/s;    201 sec\n",
      "[2022-06-22 02:56:40,047 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:40,837 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:41,114 INFO] Step 8500/100000; acc:  12.92; ppl: 52.56; xent: 3.96; lr: 0.00050; 10633/1048 tok/s;    202 sec\n",
      "[2022-06-22 02:56:41,726 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:42,434 INFO] Step 8550/100000; acc:  11.56; ppl: 42.87; xent: 3.76; lr: 0.00050; 10704/1056 tok/s;    204 sec\n",
      "[2022-06-22 02:56:42,680 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:43,699 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:43,806 INFO] Step 8600/100000; acc:  10.22; ppl: 45.38; xent: 3.82; lr: 0.00050; 10638/1063 tok/s;    205 sec\n",
      "[2022-06-22 02:56:44,546 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:45,084 INFO] Step 8650/100000; acc:  12.15; ppl: 44.81; xent: 3.80; lr: 0.00050; 10621/1051 tok/s;    206 sec\n",
      "[2022-06-22 02:56:45,515 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:46,260 INFO] Step 8700/100000; acc:  13.33; ppl: 41.01; xent: 3.71; lr: 0.00050; 12129/976 tok/s;    207 sec\n",
      "[2022-06-22 02:56:46,386 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:47,436 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:47,630 INFO] Step 8750/100000; acc:  10.40; ppl: 49.26; xent: 3.90; lr: 0.00050; 10230/1081 tok/s;    209 sec\n",
      "[2022-06-22 02:56:48,336 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:48,820 INFO] Step 8800/100000; acc:  11.34; ppl: 47.59; xent: 3.86; lr: 0.00050; 9569/1083 tok/s;    210 sec\n",
      "[2022-06-22 02:56:49,336 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:50,093 INFO] Step 8850/100000; acc:  12.39; ppl: 44.82; xent: 3.80; lr: 0.00050; 10771/1047 tok/s;    211 sec\n",
      "[2022-06-22 02:56:50,305 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:51,180 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:51,454 INFO] Step 8900/100000; acc:  10.87; ppl: 51.54; xent: 3.94; lr: 0.00050; 10483/1069 tok/s;    213 sec\n",
      "[2022-06-22 02:56:52,159 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:52,825 INFO] Step 8950/100000; acc:  10.14; ppl: 49.88; xent: 3.91; lr: 0.00050; 10857/1058 tok/s;    214 sec\n",
      "[2022-06-22 02:56:53,043 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:54,098 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:54,158 INFO] Step 9000/100000; acc:  11.63; ppl: 48.47; xent: 3.88; lr: 0.00050; 10846/1052 tok/s;    215 sec\n",
      "[2022-06-22 02:56:55,115 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:55,701 INFO] Step 9050/100000; acc:  11.13; ppl: 45.86; xent: 3.83; lr: 0.00050; 11835/920 tok/s;    217 sec\n",
      "[2022-06-22 02:56:56,020 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:57,112 INFO] Step 9100/100000; acc:  10.88; ppl: 54.18; xent: 3.99; lr: 0.00050; 10334/1082 tok/s;    218 sec\n",
      "[2022-06-22 02:56:57,211 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:58,303 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:56:58,774 INFO] Step 9150/100000; acc:  11.84; ppl: 50.74; xent: 3.93; lr: 0.00050; 11990/1052 tok/s;    220 sec\n",
      "[2022-06-22 02:56:59,292 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:00,142 INFO] Step 9200/100000; acc:  10.90; ppl: 53.16; xent: 3.97; lr: 0.00050; 10464/1087 tok/s;    221 sec\n",
      "[2022-06-22 02:57:00,454 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:01,335 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:01,362 INFO] Step 9250/100000; acc:  10.59; ppl: 52.88; xent: 3.97; lr: 0.00050; 11288/1029 tok/s;    222 sec\n",
      "[2022-06-22 02:57:01,456 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 5\n",
      "[2022-06-22 02:57:01,526 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:02,653 INFO] Step 9300/100000; acc:  10.76; ppl: 44.58; xent: 3.80; lr: 0.00050; 8971/1017 tok/s;    224 sec\n",
      "[2022-06-22 02:57:02,715 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:03,576 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:04,023 INFO] Step 9350/100000; acc:  11.93; ppl: 44.72; xent: 3.80; lr: 0.00050; 11645/1034 tok/s;    225 sec\n",
      "[2022-06-22 02:57:04,544 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:05,361 INFO] Step 9400/100000; acc:  10.48; ppl: 48.48; xent: 3.88; lr: 0.00050; 9700/1113 tok/s;    226 sec\n",
      "[2022-06-22 02:57:05,586 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:06,667 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:06,779 INFO] Step 9450/100000; acc:  10.02; ppl: 50.09; xent: 3.91; lr: 0.00050; 12021/1028 tok/s;    228 sec\n",
      "[2022-06-22 02:57:07,678 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:08,294 INFO] Step 9500/100000; acc:  11.87; ppl: 54.36; xent: 4.00; lr: 0.00050; 12352/1024 tok/s;    229 sec\n",
      "[2022-06-22 02:57:08,691 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:09,691 INFO] Step 9550/100000; acc:  10.33; ppl: 53.14; xent: 3.97; lr: 0.00050; 9863/1116 tok/s;    231 sec\n",
      "[2022-06-22 02:57:09,692 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:10,820 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:11,112 INFO] Step 9600/100000; acc:   9.36; ppl: 55.73; xent: 4.02; lr: 0.00050; 8989/1151 tok/s;    232 sec\n",
      "[2022-06-22 02:57:11,736 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:12,463 INFO] Step 9650/100000; acc:  11.20; ppl: 49.84; xent: 3.91; lr: 0.00050; 12706/992 tok/s;    234 sec\n",
      "[2022-06-22 02:57:12,645 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:13,560 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:13,705 INFO] Step 9700/100000; acc:  11.47; ppl: 46.86; xent: 3.85; lr: 0.00050; 9244/1117 tok/s;    235 sec\n",
      "[2022-06-22 02:57:14,619 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:15,158 INFO] Step 9750/100000; acc:  10.96; ppl: 48.66; xent: 3.88; lr: 0.00050; 9552/1130 tok/s;    236 sec\n",
      "[2022-06-22 02:57:15,564 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:16,576 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:16,661 INFO] Step 9800/100000; acc:  11.30; ppl: 51.14; xent: 3.93; lr: 0.00050; 11003/1077 tok/s;    238 sec\n",
      "[2022-06-22 02:57:17,547 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:18,016 INFO] Step 9850/100000; acc:  11.91; ppl: 48.79; xent: 3.89; lr: 0.00050; 10746/1073 tok/s;    239 sec\n",
      "[2022-06-22 02:57:18,787 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:19,422 INFO] Step 9900/100000; acc:   9.84; ppl: 51.29; xent: 3.94; lr: 0.00050; 9749/1121 tok/s;    241 sec\n",
      "[2022-06-22 02:57:19,858 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:20,820 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:20,880 INFO] Step 9950/100000; acc:  12.15; ppl: 55.55; xent: 4.02; lr: 0.00050; 9527/1135 tok/s;    242 sec\n",
      "[2022-06-22 02:57:21,894 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:22,142 INFO] Step 10000/100000; acc:  11.25; ppl: 44.72; xent: 3.80; lr: 0.00050; 9422/1114 tok/s;    243 sec\n",
      "[2022-06-22 02:57:22,972 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:23,624 INFO] Step 10050/100000; acc:  10.99; ppl: 49.36; xent: 3.90; lr: 0.00050; 11465/1056 tok/s;    245 sec\n",
      "[2022-06-22 02:57:24,084 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:24,854 INFO] Step 10100/100000; acc:  11.60; ppl: 48.58; xent: 3.88; lr: 0.00050; 11005/1045 tok/s;    246 sec\n",
      "[2022-06-22 02:57:25,037 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:25,979 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:26,126 INFO] Step 10150/100000; acc:  11.41; ppl: 43.66; xent: 3.78; lr: 0.00050; 9606/1096 tok/s;    247 sec\n",
      "[2022-06-22 02:57:26,865 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:27,385 INFO] Step 10200/100000; acc:  12.24; ppl: 48.76; xent: 3.89; lr: 0.00050; 11670/1012 tok/s;    248 sec\n",
      "[2022-06-22 02:57:27,648 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:28,707 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:28,743 INFO] Step 10250/100000; acc:  10.99; ppl: 43.65; xent: 3.78; lr: 0.00050; 11220/1046 tok/s;    250 sec\n",
      "[2022-06-22 02:57:29,540 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:30,012 INFO] Step 10300/100000; acc:  10.88; ppl: 47.07; xent: 3.85; lr: 0.00050; 8906/1130 tok/s;    251 sec\n",
      "[2022-06-22 02:57:30,583 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:31,445 INFO] Step 10350/100000; acc:  10.31; ppl: 50.52; xent: 3.92; lr: 0.00050; 10920/1071 tok/s;    253 sec\n",
      "[2022-06-22 02:57:31,607 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:32,552 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:32,761 INFO] Step 10400/100000; acc:  12.20; ppl: 46.26; xent: 3.83; lr: 0.00050; 13629/941 tok/s;    254 sec\n",
      "[2022-06-22 02:57:33,345 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:34,204 INFO] Step 10450/100000; acc:  10.17; ppl: 55.45; xent: 4.02; lr: 0.00050; 10284/1097 tok/s;    255 sec\n",
      "[2022-06-22 02:57:34,595 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:35,506 INFO] Step 10500/100000; acc:  10.78; ppl: 50.03; xent: 3.91; lr: 0.00050; 9395/1119 tok/s;    257 sec\n",
      "[2022-06-22 02:57:35,519 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:36,382 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:36,836 INFO] Step 10550/100000; acc:  10.98; ppl: 49.69; xent: 3.91; lr: 0.00050; 11319/1041 tok/s;    258 sec\n",
      "[2022-06-22 02:57:37,459 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:38,080 INFO] Step 10600/100000; acc:  12.25; ppl: 49.59; xent: 3.90; lr: 0.00050; 11142/1038 tok/s;    259 sec\n",
      "[2022-06-22 02:57:38,351 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:39,199 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:39,338 INFO] Step 10650/100000; acc:  11.16; ppl: 46.15; xent: 3.83; lr: 0.00050; 12116/997 tok/s;    260 sec\n",
      "[2022-06-22 02:57:39,988 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:40,657 INFO] Step 10700/100000; acc:  10.83; ppl: 53.08; xent: 3.97; lr: 0.00050; 11106/1050 tok/s;    262 sec\n",
      "[2022-06-22 02:57:40,845 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:41,834 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:42,015 INFO] Step 10750/100000; acc:  11.97; ppl: 46.65; xent: 3.84; lr: 0.00050; 12489/997 tok/s;    263 sec\n",
      "[2022-06-22 02:57:42,788 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:43,222 INFO] Step 10800/100000; acc:  10.25; ppl: 46.32; xent: 3.84; lr: 0.00050; 9569/1099 tok/s;    264 sec\n",
      "[2022-06-22 02:57:43,737 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:44,545 INFO] Step 10850/100000; acc:  11.86; ppl: 52.03; xent: 3.95; lr: 0.00050; 11775/1020 tok/s;    266 sec\n",
      "[2022-06-22 02:57:44,665 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:45,668 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:45,836 INFO] Step 10900/100000; acc:  11.45; ppl: 45.58; xent: 3.82; lr: 0.00050; 10662/1063 tok/s;    267 sec\n",
      "[2022-06-22 02:57:46,568 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:47,295 INFO] Step 10950/100000; acc:  11.46; ppl: 47.12; xent: 3.85; lr: 0.00050; 10873/1077 tok/s;    268 sec\n",
      "[2022-06-22 02:57:47,567 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:48,535 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:48,752 INFO] Step 11000/100000; acc:  10.03; ppl: 51.21; xent: 3.94; lr: 0.00050; 10371/1095 tok/s;    270 sec\n",
      "[2022-06-22 02:57:49,457 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:49,931 INFO] Step 11050/100000; acc:  10.34; ppl: 50.42; xent: 3.92; lr: 0.00050; 9828/1083 tok/s;    271 sec\n",
      "[2022-06-22 02:57:50,381 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:51,298 INFO] Step 11100/100000; acc:  11.24; ppl: 47.69; xent: 3.86; lr: 0.00050; 11624/1035 tok/s;    272 sec\n",
      "[2022-06-22 02:57:51,299 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:52,368 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:52,673 INFO] Step 11150/100000; acc:  10.71; ppl: 47.28; xent: 3.86; lr: 0.00050; 11515/1039 tok/s;    274 sec\n",
      "[2022-06-22 02:57:53,220 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:54,000 INFO] Step 11200/100000; acc:  12.81; ppl: 48.78; xent: 3.89; lr: 0.00050; 11654/1029 tok/s;    275 sec\n",
      "[2022-06-22 02:57:54,374 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:55,319 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:55,410 INFO] Step 11250/100000; acc:   9.75; ppl: 59.11; xent: 4.08; lr: 0.00050; 8749/1158 tok/s;    276 sec\n",
      "[2022-06-22 02:57:56,183 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:56,698 INFO] Step 11300/100000; acc:  11.18; ppl: 47.15; xent: 3.85; lr: 0.00050; 10476/1069 tok/s;    278 sec\n",
      "[2022-06-22 02:57:56,777 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 6\n",
      "[2022-06-22 02:57:56,844 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:57,669 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:58,065 INFO] Step 11350/100000; acc:  11.44; ppl: 46.02; xent: 3.83; lr: 0.00050; 11289/940 tok/s;    279 sec\n",
      "[2022-06-22 02:57:58,518 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:57:59,349 INFO] Step 11400/100000; acc:  12.19; ppl: 43.61; xent: 3.78; lr: 0.00050; 10144/1087 tok/s;    280 sec\n",
      "[2022-06-22 02:57:59,555 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:00,280 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:00,544 INFO] Step 11450/100000; acc:  10.43; ppl: 47.90; xent: 3.87; lr: 0.00050; 12279/980 tok/s;    282 sec\n",
      "[2022-06-22 02:58:01,323 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:01,818 INFO] Step 11500/100000; acc:  11.54; ppl: 44.62; xent: 3.80; lr: 0.00050; 9978/1088 tok/s;    283 sec\n",
      "[2022-06-22 02:58:02,225 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:03,248 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:03,263 INFO] Step 11550/100000; acc:  11.35; ppl: 50.09; xent: 3.91; lr: 0.00050; 8493/1172 tok/s;    284 sec\n",
      "[2022-06-22 02:58:04,240 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:04,657 INFO] Step 11600/100000; acc:  11.52; ppl: 49.57; xent: 3.90; lr: 0.00050; 9973/972 tok/s;    286 sec\n",
      "[2022-06-22 02:58:05,344 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:06,036 INFO] Step 11650/100000; acc:  11.91; ppl: 45.81; xent: 3.82; lr: 0.00050; 11869/1023 tok/s;    287 sec\n",
      "[2022-06-22 02:58:06,414 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:07,378 INFO] Step 11700/100000; acc:  10.73; ppl: 46.73; xent: 3.84; lr: 0.00050; 9964/1098 tok/s;    288 sec\n",
      "[2022-06-22 02:58:07,552 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:08,565 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:08,785 INFO] Step 11750/100000; acc:  11.38; ppl: 46.41; xent: 3.84; lr: 0.00050; 11415/1038 tok/s;    290 sec\n",
      "[2022-06-22 02:58:09,458 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:10,148 INFO] Step 11800/100000; acc:  11.60; ppl: 48.55; xent: 3.88; lr: 0.00050; 10043/1101 tok/s;    291 sec\n",
      "[2022-06-22 02:58:10,456 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:11,312 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:11,585 INFO] Step 11850/100000; acc:  11.64; ppl: 46.26; xent: 3.83; lr: 0.00050; 12455/999 tok/s;    293 sec\n",
      "[2022-06-22 02:58:12,245 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:12,783 INFO] Step 11900/100000; acc:  10.32; ppl: 46.51; xent: 3.84; lr: 0.00050; 10281/1068 tok/s;    294 sec\n",
      "[2022-06-22 02:58:13,167 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:14,164 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:14,200 INFO] Step 11950/100000; acc:   9.82; ppl: 49.63; xent: 3.90; lr: 0.00050; 11504/1043 tok/s;    295 sec\n",
      "[2022-06-22 02:58:15,365 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:15,658 INFO] Step 12000/100000; acc:  10.82; ppl: 51.04; xent: 3.93; lr: 0.00050; 10607/1090 tok/s;    297 sec\n",
      "[2022-06-22 02:58:16,435 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:16,983 INFO] Step 12050/100000; acc:  12.51; ppl: 47.75; xent: 3.87; lr: 0.00050; 10029/1098 tok/s;    298 sec\n",
      "[2022-06-22 02:58:17,464 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:18,386 INFO] Step 12100/100000; acc:  10.90; ppl: 47.38; xent: 3.86; lr: 0.00050; 10334/1092 tok/s;    299 sec\n",
      "[2022-06-22 02:58:18,683 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:19,635 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n",
      "[2022-06-22 02:58:19,899 INFO] Step 12150/100000; acc:  10.27; ppl: 52.45; xent: 3.96; lr: 0.00050; 10199/1108 tok/s;    301 sec\n",
      "[2022-06-22 02:58:20,595 WARNING] The batch will be filled until we reach 1,its size may exceed 4 tokens\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [240]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m report_manager \u001b[38;5;241m=\u001b[39m onmt\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mReportMgr(\n\u001b[1;32m      2\u001b[0m     report_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tensorboard_writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m onmt\u001b[38;5;241m.\u001b[39mTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m                        train_loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m      6\u001b[0m                        valid_loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m      7\u001b[0m                        optim\u001b[38;5;241m=\u001b[39moptim,\n\u001b[1;32m      8\u001b[0m                        report_manager\u001b[38;5;241m=\u001b[39mreport_manager,\n\u001b[1;32m      9\u001b[0m                        dropout\u001b[38;5;241m=\u001b[39mDROP_OUT)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalid_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalid_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALID_STEPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/trainer.py:242\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_iter, train_steps, save_checkpoint_steps, valid_iter, valid_steps)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    238\u001b[0m     normalization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(onmt\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdistributed\n\u001b[1;32m    239\u001b[0m                         \u001b[38;5;241m.\u001b[39mall_gather_list\n\u001b[1;32m    240\u001b[0m                         (normalization))\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_accumulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_decay \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_average(step)\n",
      "File \u001b[0;32m~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/trainer.py:372\u001b[0m, in \u001b[0;36mTrainer._gradient_accumulation\u001b[0;34m(self, true_batches, normalization, total_stats, report_stats)\u001b[0m\n\u001b[1;32m    369\u001b[0m     bptt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# 3. Compute loss.\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     loss, batch_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrunc_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrunc_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrunc_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/utils/loss.py:195\u001b[0m, in \u001b[0;36mLossComputeBase.__call__\u001b[0;34m(self, batch, output, attns, normalization, shard_size, trunc_start, trunc_size)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(normalization), stats\n\u001b[1;32m    194\u001b[0m batch_stats \u001b[38;5;241m=\u001b[39m onmt\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mStatistics()\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard \u001b[38;5;129;01min\u001b[39;00m shards(shard_state, shard_size):\n\u001b[1;32m    196\u001b[0m     loss, stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshard)\n\u001b[1;32m    197\u001b[0m     loss\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mfloat\u001b[39m(normalization))\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/utils/loss.py:457\u001b[0m, in \u001b[0;36mshards\u001b[0;34m(state, shard_size, eval_only)\u001b[0m\n\u001b[1;32m    454\u001b[0m         variables\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mzip\u001b[39m(torch\u001b[38;5;241m.\u001b[39msplit(state[k], shard_size),\n\u001b[1;32m    455\u001b[0m                              [v_chunk\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m v_chunk \u001b[38;5;129;01min\u001b[39;00m v_split]))\n\u001b[1;32m    456\u001b[0m inputs, grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mvariables)\n\u001b[0;32m--> 457\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vrepair/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=50, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager,\n",
    "                       dropout=DROP_OUT)\n",
    "\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=TRAIN_STEPS,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=VALID_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29040a093f9d049735cf0ac35f3fe0a5a8ac8d664da2716982ddb6534d95b6cf"
  },
  "kernelspec": {
   "display_name": "vrepair",
   "language": "python",
   "name": "vrepair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
