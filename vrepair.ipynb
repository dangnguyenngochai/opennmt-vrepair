{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What in this notebook\n",
    "\n",
    "- Running the code in `demo.ipynb` with the vulnerabilities dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.inputters.corpus import ParallelCorpus\n",
    "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
    "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
    "from onmt.utils.misc import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable logging\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "import os\n",
    "\n",
    "# Defining log path to keep track of the experiment\n",
    "LOG_PATH = 'log/log_100k_steps'\n",
    "FULL_LOG_PATH = os.getcwd() + '/' + LOG_PATH\n",
    "if os.path.exists(FULL_LOG_PATH):\n",
    "    os.remove(FULL_LOG_PATH)\n",
    "    \n",
    "init_logger(log_file=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/utils/logging.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_logger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4 # data iterater definition\n",
    "VALID_BATCH_SIZE=1 # data iterator definition\n",
    "SRC_VOCAB_SIZE=2000 # fields definition\n",
    "TGT_VOCAB_SIZE=2000 # fields definition\n",
    "SRC_SEQ_LENGTH=1000 # currently not needed as we train a dummy on preprocessed data\n",
    "TGT_SEQ_LENGTH=100 # currently not needed as we train a dummy on preprocessed data\n",
    "LEARNING_RATE=0.0005 # loss definition\n",
    "LABEL_SMOOTHING=0.1 # loss definition\n",
    "ADAM_DECAY=0.9 # loss definition\n",
    "RNN_HIDDEN=256 # model definition\n",
    "EMBEDDING=256 # model definition\n",
    "WORD_VEC=256 # model definition\n",
    "DROP_OUT=0.1 # model definition\n",
    "TRAIN_STEPS=100000\n",
    "VALID_STEPS=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the original data to bulid processing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = \"vul_data/data.vocab.src\"\n",
    "tgt_vocab_path = \"vul_data/data.vocab.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:59:19,965 INFO] Loading src vocabulary from vul_data/data.vocab.src\n",
      "[2022-06-22 02:59:20,020 INFO] Loaded src vocab has 36352 tokens.\n",
      "[2022-06-22 02:59:20,031 INFO] Loading tgt vocabulary from vul_data/data.vocab.tgt\n",
      "[2022-06-22 02:59:20,038 INFO] Loaded tgt vocab has 5924 tokens.\n"
     ]
    }
   ],
   "source": [
    "# initialize the frequency counter\n",
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 1**: Compairing the generated dictionary with the one created from the original source code of VRepair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output of VRepair log\n",
    "> [2022-05-17 20:29:10,536 INFO] Loading src vocabulary from /home/lgm/VRepair2.0/param_sweep_tgt/10_parameter_sweep/data.vocab.src \\\n",
    " [2022-05-17 20:29:10,579 INFO] Loaded src vocab has 36352 tokens. \\\n",
    "[2022-05-17 20:29:10,588 INFO] Loading tgt vocabulary from /home/lgm/VRepair2.0/param_sweep_tgt/10_parameter_sweep/data.vocab.tgt \\\n",
    "[2022-05-17 20:29:10,594 INFO] Loaded tgt vocab has 5924 tokens. \\\n",
    "[2022-05-17 20:29:10,596 INFO] Building fields with vocab in counters... \\\n",
    "[2022-05-17 20:29:10,599 INFO]  * tgt vocab size: 5004. \\\n",
    "[2022-05-17 20:29:10,631 INFO]  * src vocab size: 5002. \\\n",
    "[2022-05-17 20:29:10,632 INFO]  * src vocab size = 5002 \\\n",
    "[2022-05-17 20:29:10,632 INFO]  * tgt vocab size = 5004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS RQ1:** The vocab generated is the same with previous experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:59:22,450 INFO]  * tgt vocab size: 2004.\n",
      "[2022-06-22 02:59:22,470 INFO]  * src vocab size: 2002.\n"
     ]
    }
   ],
   "source": [
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = SRC_VOCAB_SIZE\n",
    "tgt_vocab_size = TGT_VOCAB_SIZE\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0m_build_fields_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcounters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshare_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocab_size_multiple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_words_min_frequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_words_min_frequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_specials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtgt_specials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/inputters/inputter.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_build_fields_vocab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and optimizer creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the field vocab is used instead of the origial vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab \n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMTLossCompute(\n",
       "  (criterion): LabelSmoothingLoss()\n",
       "  (generator): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2004, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = EMBEDDING\n",
    "rnn_size = RNN_HIDDEN\n",
    "# Specify the core model.\n",
    "\n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
    "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
    "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=onmt.utils.loss.LabelSmoothingLoss(ignore_index=tgt_padding, label_smoothing=LABEL_SMOOTHING, tgt_vocab_size=len(tgt_vocab)),\n",
    "    generator=model.generator)\n",
    "loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from onmt.opts import dynamic_prepare_opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from onmt.constants import ModelTask\n",
    "parser = ArgumentParser(description='build_loss_compute')\n",
    "\n",
    "base_args = ([\"-copy_attn\", \"True\" , \"-label_smoothing\", str(LABEL_SMOOTHING), \"-model_task\", ModelTask.SEQ2SEQ])\n",
    "opts, unknown = parser.parse_known_args(base_args)\n",
    "loss = onmt.utils.loss.build_loss_compute(model, tgt_field=tgt_text_field, opt=opts, train=True)\n",
    "valid_loss = onmt.utils.loss.build_loss_compute(model, tgt_field=tgt_text_field, opt=opts, train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LEARNING_RATE\n",
    "torch_optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=ADAM_DECAY)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(2002, 256, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(256, 128, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(2004, 256, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 256)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (linear_out): Linear(in_features=512, out_features=256, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=2004, bias=True)\n",
      "    (1): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"vul_data/random_fine_tune_train.src.txt\"\n",
    "tgt_train = \"vul_data/random_fine_tune_train.tgt.txt\"\n",
    "src_val = \"vul_data/random_fine_tune_valid.src.txt\"\n",
    "tgt_val = \"vul_data/random_fine_tune_valid.tgt.txt\"\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(\"corpus\", src_train, tgt_train)\n",
    "valid = ParallelCorpus(\"valid\", src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the training iterator\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora={\"corpus\": corpus},\n",
    "    corpora_info={\"corpus\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=True,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDynamicDatasetIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpora_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size_multiple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbucket_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpool_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_empty_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Yield batch from (multiple) plain text corpus.\n",
       "\n",
       "Args:\n",
       "    corpora (dict[str, ParallelCorpus]): collections of corpora to iterate;\n",
       "    corpora_info (dict[str, dict]): corpora infos correspond to corpora;\n",
       "    transforms (dict[str, Transform]): transforms may be used by corpora;\n",
       "    fields (dict[str, Field]): fields dict for convert corpora into Tensor;\n",
       "    is_train (bool): True when generate data for training;\n",
       "    batch_type (str): batching type to count on, choices=[tokens, sents];\n",
       "    batch_size (int): numbers of examples in a batch;\n",
       "    batch_size_multiple (int): make batch size multiply of this;\n",
       "    data_type (str): input data type, currently only text;\n",
       "    bucket_size (int): accum this number of examples in a dynamic dataset;\n",
       "    pool_factor (int): accum this number of batch before sorting;\n",
       "    skip_empty_level (str): security level when encouter empty line;\n",
       "    stride (int): iterate data files with this stride;\n",
       "    offset (int): iterate data files with this offset.\n",
       "\n",
       "Attributes:\n",
       "    batch_size_fn (function): functions to calculate batch_size;\n",
       "    sort_key (function): functions define how to sort examples;\n",
       "    dataset_adapter (DatasetAdapter): organize raw corpus to tensor adapt;\n",
       "    mixer (MixingStrategy): the strategy to iterate corpora.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/vrepair/lib/python3.10/site-packages/onmt/inputters/dynamic_iterator.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DynamicDatasetIter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
    "train_iter = iter(IterOnDevice(train_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation iterator\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora={\"valid\": valid},\n",
    "    corpora_info={\"valid\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=False,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = IterOnDevice(valid_iter, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-22 02:59:37,722 INFO] Start training loop and validate every 20000 steps...\n",
      "[2022-06-22 02:59:37,723 INFO] corpus's transforms: TransformPipe()\n",
      "[2022-06-22 02:59:37,723 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 1\n",
      "[2022-06-22 02:59:40,291 INFO] Step 50/100000; acc:  11.37; ppl: 210.14; xent: 5.35; lr: 0.00050; 21252/2398 tok/s;      3 sec\n",
      "[2022-06-22 02:59:42,465 INFO] Step 100/100000; acc:  11.70; ppl: 52.43; xent: 3.96; lr: 0.00050; 28169/2575 tok/s;      5 sec\n",
      "[2022-06-22 02:59:44,599 INFO] Step 150/100000; acc:  11.63; ppl: 49.46; xent: 3.90; lr: 0.00050; 28154/2604 tok/s;      7 sec\n",
      "[2022-06-22 02:59:46,659 INFO] Step 200/100000; acc:  11.84; ppl: 46.34; xent: 3.84; lr: 0.00050; 26642/2641 tok/s;      9 sec\n",
      "[2022-06-22 02:59:48,841 INFO] Step 250/100000; acc:  11.62; ppl: 47.70; xent: 3.86; lr: 0.00050; 27344/2624 tok/s;     11 sec\n",
      "[2022-06-22 02:59:50,944 INFO] Step 300/100000; acc:  11.14; ppl: 48.47; xent: 3.88; lr: 0.00050; 25877/2667 tok/s;     13 sec\n",
      "[2022-06-22 02:59:53,218 INFO] Step 350/100000; acc:  12.30; ppl: 48.43; xent: 3.88; lr: 0.00050; 26961/2717 tok/s;     15 sec\n",
      "[2022-06-22 02:59:55,141 INFO] Step 400/100000; acc:  13.68; ppl: 43.35; xent: 3.77; lr: 0.00050; 25791/2791 tok/s;     17 sec\n",
      "[2022-06-22 02:59:57,244 INFO] Step 450/100000; acc:  12.71; ppl: 44.62; xent: 3.80; lr: 0.00050; 30867/2546 tok/s;     20 sec\n",
      "[2022-06-22 02:59:59,498 INFO] Step 500/100000; acc:  13.07; ppl: 46.14; xent: 3.83; lr: 0.00050; 26457/2662 tok/s;     22 sec\n",
      "[2022-06-22 03:00:00,041 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 2\n",
      "[2022-06-22 03:00:01,914 INFO] Step 550/100000; acc:  13.87; ppl: 43.86; xent: 3.78; lr: 0.00050; 23566/2615 tok/s;     24 sec\n",
      "[2022-06-22 03:00:04,204 INFO] Step 600/100000; acc:  15.44; ppl: 41.26; xent: 3.72; lr: 0.00050; 25107/2616 tok/s;     26 sec\n",
      "[2022-06-22 03:00:06,348 INFO] Step 650/100000; acc:  14.79; ppl: 44.22; xent: 3.79; lr: 0.00050; 28245/2517 tok/s;     29 sec\n",
      "[2022-06-22 03:00:08,425 INFO] Step 700/100000; acc:  14.93; ppl: 41.35; xent: 3.72; lr: 0.00050; 27039/2597 tok/s;     31 sec\n",
      "[2022-06-22 03:00:10,515 INFO] Step 750/100000; acc:  15.93; ppl: 40.22; xent: 3.69; lr: 0.00050; 26910/2731 tok/s;     33 sec\n",
      "[2022-06-22 03:00:12,667 INFO] Step 800/100000; acc:  15.39; ppl: 41.91; xent: 3.74; lr: 0.00050; 25732/2688 tok/s;     35 sec\n",
      "[2022-06-22 03:00:14,908 INFO] Step 850/100000; acc:  16.70; ppl: 41.15; xent: 3.72; lr: 0.00050; 28520/2644 tok/s;     37 sec\n",
      "[2022-06-22 03:00:17,038 INFO] Step 900/100000; acc:  15.70; ppl: 40.87; xent: 3.71; lr: 0.00050; 24785/2844 tok/s;     39 sec\n",
      "[2022-06-22 03:00:19,107 INFO] Step 950/100000; acc:  17.13; ppl: 38.77; xent: 3.66; lr: 0.00050; 26936/2527 tok/s;     41 sec\n",
      "[2022-06-22 03:00:21,151 INFO] Step 1000/100000; acc:  17.21; ppl: 38.70; xent: 3.66; lr: 0.00050; 29660/2628 tok/s;     43 sec\n",
      "[2022-06-22 03:00:22,292 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 3\n",
      "[2022-06-22 03:00:23,611 INFO] Step 1050/100000; acc:  15.36; ppl: 39.02; xent: 3.66; lr: 0.00050; 23101/2501 tok/s;     46 sec\n",
      "[2022-06-22 03:00:25,785 INFO] Step 1100/100000; acc:  14.97; ppl: 40.82; xent: 3.71; lr: 0.00050; 25329/2806 tok/s;     48 sec\n",
      "[2022-06-22 03:00:28,252 INFO] Step 1150/100000; acc:  16.36; ppl: 41.18; xent: 3.72; lr: 0.00050; 27479/2604 tok/s;     51 sec\n",
      "[2022-06-22 03:00:30,391 INFO] Step 1200/100000; acc:  16.16; ppl: 39.72; xent: 3.68; lr: 0.00050; 26815/2717 tok/s;     53 sec\n",
      "[2022-06-22 03:00:32,383 INFO] Step 1250/100000; acc:  14.98; ppl: 40.39; xent: 3.70; lr: 0.00050; 25468/2792 tok/s;     55 sec\n",
      "[2022-06-22 03:00:34,418 INFO] Step 1300/100000; acc:  16.15; ppl: 39.60; xent: 3.68; lr: 0.00050; 27008/2649 tok/s;     57 sec\n",
      "[2022-06-22 03:00:36,701 INFO] Step 1350/100000; acc:  16.19; ppl: 39.78; xent: 3.68; lr: 0.00050; 29196/2508 tok/s;     59 sec\n",
      "[2022-06-22 03:00:38,919 INFO] Step 1400/100000; acc:  15.39; ppl: 39.05; xent: 3.66; lr: 0.00050; 25325/2670 tok/s;     61 sec\n",
      "[2022-06-22 03:00:41,077 INFO] Step 1450/100000; acc:  17.10; ppl: 39.87; xent: 3.69; lr: 0.00050; 24379/2690 tok/s;     63 sec\n",
      "[2022-06-22 03:00:43,097 INFO] Step 1500/100000; acc:  16.50; ppl: 38.65; xent: 3.65; lr: 0.00050; 30715/2614 tok/s;     65 sec\n",
      "[2022-06-22 03:00:44,621 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus: 4\n",
      "[2022-06-22 03:00:45,289 INFO] Step 1550/100000; acc:  16.71; ppl: 37.93; xent: 3.64; lr: 0.00050; 26727/2558 tok/s;     68 sec\n",
      "[2022-06-22 03:00:47,430 INFO] Step 1600/100000; acc:  16.80; ppl: 37.55; xent: 3.63; lr: 0.00050; 26116/2720 tok/s;     70 sec\n",
      "[2022-06-22 03:00:49,614 INFO] Step 1650/100000; acc:  16.18; ppl: 39.76; xent: 3.68; lr: 0.00050; 29603/2794 tok/s;     72 sec\n",
      "[2022-06-22 03:00:51,854 INFO] Step 1700/100000; acc:  15.87; ppl: 41.60; xent: 3.73; lr: 0.00050; 26077/2538 tok/s;     74 sec\n"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=50, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager,\n",
    "                       dropout=DROP_OUT)\n",
    "\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=TRAIN_STEPS,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=VALID_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29040a093f9d049735cf0ac35f3fe0a5a8ac8d664da2716982ddb6534d95b6cf"
  },
  "kernelspec": {
   "display_name": "vrepair",
   "language": "python",
   "name": "vrepair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
